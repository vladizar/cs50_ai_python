This readme is made as a diary :)
First of all, I copy pasted the convolutional NN model code from lecture notes with needed changes correpsonding to the problem just to feel like I have something working. The accuracy was 0.9643, so at that moment I decided to increase it to 0.99-0.995+.
After taking a look at images in dataset I decided that maxpooling is not useful here, because it's slightly hard even to me, human, to classify the sign on 15x15 picture. NN training time increased twice I think. But the accuracy droppped to 0.8905. Hmmm, ok, maxpooling was useful to make more generalization for NN. So maybe it'll be good to increase it? Let's try. I've changed pool size to 3x3, training speed increased 3 times I think, but accuracy is now 0.8151... I've just tried 2.5x2.5, accuracy is 0.4794... And 1.5x1.5 is 0.8841... Setting 2x2 back... And it's accuracy is now 0.4786, but after trying to run on small dataset first accuracy increased to 0.9603... IDK how it works but I should try this with others pool sizes of course. Ok, 2x2 is the best :)
It's dropout time now. I think that 0.5 is too high, so I'll decrease it to 0.3 at first. The accuracy didn't change, but bigger dropout is better than smaller, so 0.5 is the best I think :)
Convolutional layers time. I think that adding one convo layer after pooling is a good idea as it would provide more generalization maybe... Hmmm, no, it's 0.9516 now... Maybe one layer before pooling but with 64 units? Still 0.9619... Two layers: first with 64, second after pooling with 32... 0.9222
I think I need more generalization, because in small dataset accuracy is 1.0 and on big on last epoche accuracy is 0.99 but on testing data it may go down to 0.92. I found that it can be done using 'kernel_regularizer' parameter, so I'll try some of them applying to different layers of NN. Doesn't help. Just have read about batch normalization, hope it'll help... Woww, my first accuracy increaseing - 0.9786.
Tried to change dropout to 0.7... OK, 0.5 is the best possible I think. Added second normalization between pooling and flattening... 0.9825, I'm getting closer and closer to the goal! Adding normalization to other places didn't get any better, even worse.
Ok, after all I understood that almost every my of my changes wasn't stable (like it could be 0.98 at first and 0.94 the second time). So, I started to search for the best convolutional NNs models.
After playing a bit with the LeNet-5 NN (I didn't describe those all steps cause it's late night and I'm even tired of typing this letters) I somehow got the stable 0.9714-0.9976 accuracy, I've reached my goal, so I'll stop at it... I don't want to make a very deep model because the light model with 0.9 accuracy is better than heavy with 0.95 for me right now I think...

CONCLUSION
What I've learned doing this. Batch normalization is a really cool thing. It's very effective to add more levels of convo + pool layers. Dropout 0.5 is very good for 128 units hidden layer. The best pooling size is 2x2. Convo + pool is also effective for generalization. It's better to test accuracy several times. Bat—Åh normalization is useless just before and after the hidden layer. It's not effective to make 2+ hidden layers. High resolution picture is worse for input to hidden layer than low resolution. It's good to decrease resolution slowly, step by step.